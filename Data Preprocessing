import spacy
from spacy.tokens import DocBin
import nltk
from nltk.corpus import stopwords

# Ensure NLTK data is downloaded (run once)
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Load your dataset (assuming a structured format like JSON or CSV)
# Example: preprocess each sentence
def preprocess_text(text):
    # Lowercase the text
    text = text.lower()
    # Tokenize (you can also use SpaCy's tokenizer)
    tokens = text.split()
    # Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    return " ".join(tokens)

# Example usage with a sample sentence:
sample_text = "Apple is looking at buying U.K. startup for $1 billion."
preprocessed_text = preprocess_text(sample_text)
print(preprocessed_text)
